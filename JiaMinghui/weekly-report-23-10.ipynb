{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å·¥ä½œå‘¨æŠ¥\n",
    "\n",
    "## 2023.10.22\n",
    "\n",
    "### ã€æœ¬å‘¨å·¥ä½œæ€»ç»“ã€‘\n",
    "1. è°ƒç ”æ—¶é—´åºåˆ— tokenizer æ–¹å¼é€‚é…å¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚\n",
    "\n",
    "Date|Method|Conference| Paper Title and Paper Interpretation (In Chinese) |Code\n",
    "-----|----|-----|-----|-----\n",
    "| 23-02-23 | [FPT](https://arxiv.org/abs/2302.11939) ğŸŒŸ | NIPS 2023 | One Fits All:Power General Time Series Analysis by Pretrained LM | [One-Fits-All](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All)   |\n",
    "| 23-05-17 | [LLMTime](https://arxiv.org/abs/2310.07820) | NIPS 2023 | Large Language Models Are Zero-Shot Time Series Forecasters | [LLMTime](https://github.com/ngruver/llmtime) |\n",
    "| 23-08-16 | [TEST](https://arxiv.org/abs/2308.08241) | Arxiv 2023 | TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series | None |\n",
    "| 23-08-16 | [LLM4TS](https://arxiv.org/abs/2308.08469) | Arxiv 2023 | LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs | None |\n",
    "| 23-10-03 | [Time-LLM](https://arxiv.org/abs/2310.01728) | Arxiv 2023 | Time-LLM: Time Series Forecasting by Reprogramming Large Language Models | None |\n",
    "| 23-10-08 | [TEMPO](https://arxiv.org/abs/2310.04948) | Arxiv 2023 | TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting | None |\n",
    "| 23-10-12 | [Lag-Llama](https://arxiv.org/abs/2310.08278) | Arxiv 2023 | Lag-Llama: Towards Foundation Models for Time Series Forecasting | [Lag-Llama](https://github.com/kashif/pytorch-transformer-ts) |\n",
    "| 23-10-15 | [UniTime](https://arxiv.org/abs/2310.09751) | Arxiv 2023 | UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting | None |\n",
    "\n",
    "2. å¤ç°ä¸¤ç¯‡ time series forcasting è®ºæ–‡ï¼Œä½¿ç”¨è®¡ç®—æœºçš„ ETT æ•°æ®é›†ï¼Œä¸€ç¯‡æ˜¯åŸºäº LLM çš„ï¼Œä¸€ç¯‡æ˜¯åŸºäºå·ç§¯çš„ SOTA æ–¹æ³•ï¼Œå¾—åˆ°äº†å’Œè®ºæ–‡æ±‡æŠ¥å·®ä¸å¤šçš„æŒ‡æ ‡ç»“æœã€‚\n",
    "\n",
    "| Method | Conference | Paper Title |\n",
    "| ---- | ---- | ---- |\n",
    "| FPT | NIPS 2023 | [One Fits All:Power General Time Series Analysis by Pretrained LM](https://arxiv.org/abs/2302.11939) | \n",
    "| TimesNet | ICLR 2023 | [TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis](https://arxiv.org/abs/2210.02186) |\n",
    "\n",
    "### ã€ä¸‹å‘¨å·¥ä½œè®¡åˆ’ã€‘\n",
    "1. æŠ½å–æ‰‹ä¸Š 120w+ æ¡ Kepler å…‰å˜æ›²çº¿çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œæ¸…æ´—æ•´ç†æ±‡é›†ï¼Œä¾¿äºåç»­æ¨¡å‹è®­ç»ƒï¼Œé¿å…è¶…å¤§è§„æ¨¡çš„é›¶æ•£å°æ–‡ä»¶åŠ è½½æ‹–æ…¢è®­ç»ƒæ—¶é—´ã€‚\n",
    "2. è°ƒæ•´ Kepler æ›²çº¿æ•°æ®é›†é€‚é…è®¡ç®—æœºçš„æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ï¼ŒLLM æ¨¡å‹æ˜¯å¦ä¼šå–å¾—ä¸é”™çš„æ•ˆæœã€‚å¦‚æœå¯ä»¥ï¼Œæ›´å¤§çš„ LLM å’Œè‡ªç›‘ç£çš„æ–¹æ³•æ˜¯å¦å¯ä»¥å–å¾—æ›´å¥½çš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023.10.23 - 2023.10.29\n",
    "\n",
    "### ã€æœ¬å‘¨å·¥ä½œæ€»ç»“ã€‘\n",
    "1. æŠ½å–æ‰‹ä¸Š 120w+ æ¡ Kepler å…‰å˜æ›²çº¿çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œæ¸…æ´—æ•´ç†æ±‡é›†ã€‚è¿™æ ·çš„æ•°æ®é‡åœ¨æ•´ä¸ªè®¡ç®—æœºç•Œéƒ½æ˜¯ç¨€ç¼ºçš„ï¼Œå…ˆç”¨ä¸€éƒ¨åˆ†å®éªŒï¼Œæœªæ¥è€ƒè™‘å°†æ•´ä¸ªkepleråšæˆä¾¿äºåŠ è½½çš„æ•°æ®é›†ï¼Œæµ‹è¯•å¤šç§æ–¹æ³•ï¼Œå¼„ä¸€ä¸ªbenchmarkingã€‚\n",
    "\n",
    "### ã€ä¸‹å‘¨å·¥ä½œè®¡åˆ’ã€‘\n",
    "1. GPT(6)æ¨¡å‹åœ¨ Kepler æ•°æ®é›†ä¸Šæµ‹è¯•ï¼Œé¢„è®­ç»ƒå’Œéšæœºåˆå§‹åŒ–å¯¹æ¯”æ•ˆæœï¼Œç›®çš„ï¼šLLM å†»ç»“æƒé‡è¦æ¯”éšæœºåˆå§‹åŒ–è¡¨ç°å¥½ï¼Œè¿™çœŸçš„æ˜¯å› ä¸ºæ¨¡å‹é‡Œæœ‰çŸ¥è¯†å—ï¼Œè¿˜æ˜¯è¯´åªæ˜¯ç°æœ‰æ•°æ®é›†ä¸å¤Ÿå¤§ï¼Œéšæœºåˆå§‹åŒ–çš„æ¨¡å‹æ²¡æœ‰å……åˆ†è®­ç»ƒï¼Œæ‰€ä»¥è¡¨ç°ä¸å¥½ã€‚\n",
    "2. è°ƒç ” irregular time series analysis çš„è®ºæ–‡ï¼Œè¿™æ˜¯ä¹‹å‰æ¼æ‰çš„ä¸€ä¸ªç ”ç©¶æ–¹å‘ï¼Œæ¯”è¾ƒå°ä¼—ï¼Œå¯¹äºå¤©æ–‡è§‚æµ‹é‡‡æ ·ä¸å‡åŒ€çš„ç‰¹ç‚¹æ¥è¯´ï¼Œå¯èƒ½ä¼šæœ‰ä½œç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script processes astronomical light curve data stored in FITS files, \n",
    "    leveraging multiprocessing for enhanced performance.\n",
    "\n",
    "It normalizes the time and flux data and saves the processed data into CSV files.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(file_path, save_root):\n",
    "    \"\"\"\n",
    "    Process a single FITS file: read, normalize data, and save as CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fits.open(file_path) as f:\n",
    "            # Extracting essential information from the header\n",
    "            header = f[0].header\n",
    "            kepid = header.get(\"KEPLERID\")\n",
    "            quarter = header.get(\"QUARTER\")\n",
    "\n",
    "            # Time normalization\n",
    "            bkjd = f[1].data[\"TIME\"]\n",
    "            bkjd_nan = np.isnan(bkjd)\n",
    "            bkjd[bkjd_nan] = np.interp(np.flatnonzero(bkjd_nan), np.flatnonzero(~bkjd_nan), bkjd[~bkjd_nan])\n",
    "            jd = bkjd + 2454833.0  # Convert BKJD to JD\n",
    "            date = Time(jd, format=\"jd\").to_datetime()\n",
    "            date = pd.to_datetime(pd.Series(date)).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            # Flux normalization\n",
    "            flux = f[1].data[\"PDCSAP_FLUX\"]\n",
    "            median = np.nanmedian(flux)\n",
    "            q1 = np.nanpercentile(flux, 25)\n",
    "            q3 = np.nanpercentile(flux, 75)\n",
    "            iqr = q3 - q1\n",
    "            iqr = 1 if iqr == 0 else iqr  # Avoid division by zero\n",
    "            flux = (flux - median) / iqr  # Scaling\n",
    "\n",
    "            # Preparing DataFrame and saving as CSV\n",
    "            df = pd.DataFrame({\"date\": date, \"flux\": flux})\n",
    "            df.set_index(\"date\", inplace=True)\n",
    "            dir_structure = os.path.dirname(file_path).split(os.sep)[-2:]\n",
    "            save_dir = os.path.join(save_root, *dir_structure)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(save_dir, f\"{str(kepid).zfill(9)}-{str(quarter).zfill(2)}.csv\")\n",
    "            df.to_csv(save_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}. Error: {e}\")\n",
    "\n",
    "def main(root, save_root):\n",
    "    \"\"\"\n",
    "    Execute the processing of FITS files using multiprocessing.\n",
    "    \"\"\"\n",
    "    files_to_process = []\n",
    "\n",
    "    # Walk through the directory tree and collect all files to process\n",
    "    for dir, subdirs, files in os.walk(root):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(dir, file)\n",
    "            files_to_process.append((file_path, save_root))\n",
    "\n",
    "    # Multiprocessing execution setup\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # This part is adjusted to work with tqdm, ensuring that the progress bar is displayed correctly.\n",
    "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "        # Prepare the progress bar\n",
    "        progress_bar = tqdm(total=len(files_to_process), desc=\"Processing files\", unit=\"file\")\n",
    "\n",
    "        # We use a list to keep all futures, and we use it to know when a future is completed.\n",
    "        futures = [executor.submit(process_file, *task) for task in files_to_process]\n",
    "\n",
    "        for _ in as_completed(futures):  # as each future completes, it returns the result.\n",
    "            progress_bar.update(1)  # Update the progress bar by 1 for each completed task.\n",
    "\n",
    "        progress_bar.close()  # Don't forget to close the progress bar at the end.\n",
    "\n",
    "# Paths for the source and destination directories\n",
    "root = \"/media/jmh/T7/kepler_lc\"\n",
    "save_root = \"/home/jmh/Datasets/kepler_csv/\"\n",
    "\n",
    "# Running the main processing function\n",
    "main(root, save_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed Compressed /home/jmh/Datasets/kepler_csv/0060 into /home/jmh/Datasets/kepler_csv_compressed/0060.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [4:51:30<00:00, 146.98s/dir]  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script: Compress Subdirectories\n",
    "\n",
    "Description:\n",
    "    This script is used to compress the subdirectories located in a specified \"original\" root directory.\n",
    "    Each subdirectory corresponds to a 'sub_id' and is compressed into an individual .tar.gz file.\n",
    "    The resulting compressed files are saved in a \"new\" specified root directory.\n",
    "\n",
    "    The script assumes that directories under the original root are structured as:\n",
    "    original_root/sub_id\n",
    "\n",
    "    The script does not recursively compress subdirectories beyond the immediate children\n",
    "    of the original root directory. The script only targets directories, ignoring any\n",
    "    files present in the original root directory.\n",
    "\n",
    "Usage:\n",
    "    The script is executed in a Python environment and does not require any command-line arguments.\n",
    "    Users must modify the 'original_root_dir' and 'new_root_dir' variables to set the paths\n",
    "    for source and destination directories, respectively.\n",
    "\n",
    "    original_root_dir = \"/path/to/original/root\"  # path to the source directory\n",
    "    new_root_dir = \"/path/to/new/root\"  # path to the destination directory\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compress_directory(sub_id_path, new_root):\n",
    "    \"\"\"\n",
    "    Compress the contents of 'sub_id_path' and save it in 'new_root'.\n",
    "\n",
    "    :param sub_id_path: Full path to the subdirectory.\n",
    "    :param new_root: Directory where the .tar.gz files will be saved.\n",
    "    \"\"\"\n",
    "    sub_id = os.path.basename(sub_id_path)\n",
    "    tar_path = os.path.join(new_root, f\"{sub_id}.tar.gz\")\n",
    "\n",
    "    # Open tarball file for writing with gzip compression\n",
    "    with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "        # Walk through each file in the sub_id directory\n",
    "        for dirpath, dirnames, filenames in os.walk(sub_id_path):\n",
    "            for file in filenames:\n",
    "                # Construct the absolute path of the file\n",
    "                absolute_file_path = os.path.join(dirpath, file)\n",
    "\n",
    "                # Identify the relative path of the file, which should be the path within the tarball\n",
    "                # This step is crucial to avoid including unnecessary path information in the tarball\n",
    "                arcname = os.path.relpath(absolute_file_path, start=sub_id_path)\n",
    "\n",
    "                # Add the file with its relative path information to the tarball\n",
    "                tar.add(absolute_file_path, arcname=arcname)\n",
    "\n",
    "    return f\"Compressed {sub_id_path} into {tar_path}\"  # Return status\n",
    "\n",
    "\n",
    "def main(original_root, new_root):\n",
    "    # Create new root if it doesn't exist.\n",
    "    if not os.path.exists(new_root):\n",
    "        os.makedirs(new_root)\n",
    "\n",
    "    # Construct a list of directories to be compressed\n",
    "    dirs_to_compress = [os.path.join(original_root, sub_id) for sub_id in os.listdir(original_root) \\\n",
    "                         if os.path.isdir(os.path.join(original_root, sub_id))]\n",
    "    \n",
    "    # Number of directories to compress, for the progress bar\n",
    "    num_dirs = len(dirs_to_compress)\n",
    "\n",
    "    # We use a ThreadPoolExecutor to compress directories in separate threads.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        # Setup tqdm to track the task's progress\n",
    "        progress = tqdm(\n",
    "            concurrent.futures.as_completed([\n",
    "                executor.submit(compress_directory, dir_path, new_root) for dir_path in dirs_to_compress\n",
    "            ]), \n",
    "            total=num_dirs, \n",
    "            unit=\"dir\", \n",
    "            desc=\"Compressing\", \n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        for future in progress:\n",
    "            try:\n",
    "                # Fetch the result and update the progress bar description\n",
    "                result = future.result()\n",
    "                progress.set_description(f\"Processed {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred: {e}\")\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress.close()\n",
    "\n",
    "# Set your original and new root directories.\n",
    "original_root_dir = \"/home/jmh/Datasets/kepler_csv/\"\n",
    "new_root_dir = \"/home/jmh/Datasets/kepler_csv_compressed/\"\n",
    "\n",
    "main(original_root_dir, new_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/1282116 [00:00<?, ?file/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-Process Time Series Data Cleaning Script\n",
    "\n",
    "This script is designed to process large sets of time series data stored in CSV files.\n",
    "It performs the following operations:\n",
    "1. Removes any rows with NaNs at the beginning and end of the time series data.\n",
    "2. Applies third-order spline interpolation for missing values within the series.\n",
    "3. Saves the cleaned data back to CSV, replacing the original files.\n",
    "\n",
    "The script leverages multiprocessing for enhanced performance, particularly when dealing\n",
    "with a large number of files. It also implements progress tracking, giving real-time feedback\n",
    "on the number of files processed.\n",
    "\n",
    "Usage:\n",
    "    Define the root directory containing the CSV files and the save directory for processed files.\n",
    "    Run the script, and it will process files located in 'root/sub_id/id/*.csv'.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import splrep, splev\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_time_series(file_path, save_root):\n",
    "    \"\"\"\n",
    "    Processes a single time series CSV file, handling missing data and saving the modified file.\n",
    "    \n",
    "    :param file_path: Path to the original CSV file.\n",
    "    :param save_root: Root directory where the processed file will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and prepare the data\n",
    "        data = pd.read_csv(file_path)\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data.set_index('date', inplace=True)\n",
    "        data.sort_index(inplace=True)\n",
    "        \n",
    "        # 1. Remove NaNs at the beginning and the end\n",
    "        # Identify valid data points (non-NaN) from start and end\n",
    "        valid_start = data['flux'].first_valid_index()\n",
    "        valid_end = data['flux'].last_valid_index()\n",
    "\n",
    "        # Drop the rows with NaNs at the beginning and end of the dataframe\n",
    "        if valid_start is not None and valid_end is not None:\n",
    "            data = data.loc[valid_start:valid_end]\n",
    "        else:\n",
    "            # If the entire series is NaN, we skip the file\n",
    "            print(f\"File {file_path} was skipped as it contains all NaNs.\")\n",
    "            return\n",
    "        \n",
    "        # 2. Spline interpolation for NaNs in the middle of the series\n",
    "        # Identify where the NaNs are in the series\n",
    "        nan_positions = data['flux'].isna()\n",
    "\n",
    "        # If there are NaNs to interpolate\n",
    "        if nan_positions.any():\n",
    "            # Extract the positions (indices) and values for non-NaN elements\n",
    "            x = np.where(~nan_positions)[0]\n",
    "            y = data.loc[~nan_positions, 'flux']\n",
    "\n",
    "            # Create a spline of degree 3 (cubic spline)\n",
    "            spline = splrep(x, y, k=3)\n",
    "\n",
    "            # Identify the positions where we have NaNs\n",
    "            x_interp_positions = np.where(nan_positions)[0]\n",
    "\n",
    "            # Calculate the interpolated values at these positions\n",
    "            interpolated_values = splev(x_interp_positions, spline)\n",
    "\n",
    "            # Replace NaNs with interpolated values in the dataframe\n",
    "            data.loc[nan_positions, 'flux'] = interpolated_values\n",
    "        \n",
    "        # Construct the save path from the original file path structure\n",
    "        dir_structure = os.path.dirname(file_path).split(os.sep)[-2:]\n",
    "        save_dir = os.path.join(save_root, *dir_structure)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        save_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "        # Save the cleaned data\n",
    "        data.to_csv(save_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "def main(root, save_root):\n",
    "    \"\"\"\n",
    "    Orchestrates the multiprocessing execution for cleaning time series data.\n",
    "    \n",
    "    :param root: Root directory containing the unprocessed CSV files.\n",
    "    :param save_root: Root directory where processed files will be stored.\n",
    "    \"\"\"\n",
    "    # Retrieve file paths\n",
    "    files_to_process = [os.path.join(dir, file) \n",
    "                        for dir, _, files in os.walk(root) for file in files if file.endswith(\".csv\")]\n",
    "\n",
    "    # Setup for multiprocessing\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tqdm(total=len(files_to_process), desc=\"Processing files\", unit=\"file\")\n",
    "\n",
    "        # Submit tasks to the process pool\n",
    "        futures = [executor.submit(process_time_series, file_path, save_root) for file_path in files_to_process]\n",
    "\n",
    "        # Update progress for each completed task\n",
    "        for _ in as_completed(futures):\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.refresh()\n",
    "\n",
    "        # Cleanup\n",
    "        progress_bar.close() \n",
    "\n",
    "# Specify source and destination directories\n",
    "root = \"/home/jmh/Datasets/kepler/\"  # Root directory with original CSV files\n",
    "save_root = \"/home/jmh/Datasets/kepler_filled/\"  # Destination for processed data\n",
    "\n",
    "# Start the processing\n",
    "main(root, save_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
