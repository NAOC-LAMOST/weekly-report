{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工作周报\n",
    "\n",
    "## 2023.10.22\n",
    "\n",
    "### 【本周工作总结】\n",
    "1. 调研时间序列 tokenizer 方式适配大语言模型的方法。\n",
    "\n",
    "Date|Method|Conference| Paper Title and Paper Interpretation (In Chinese) |Code\n",
    "-----|----|-----|-----|-----\n",
    "| 23-02-23 | [FPT](https://arxiv.org/abs/2302.11939) 🌟 | NIPS 2023 | One Fits All:Power General Time Series Analysis by Pretrained LM | [One-Fits-All](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All)   |\n",
    "| 23-05-17 | [LLMTime](https://arxiv.org/abs/2310.07820) | NIPS 2023 | Large Language Models Are Zero-Shot Time Series Forecasters | [LLMTime](https://github.com/ngruver/llmtime) |\n",
    "| 23-08-16 | [TEST](https://arxiv.org/abs/2308.08241) | Arxiv 2023 | TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series | None |\n",
    "| 23-08-16 | [LLM4TS](https://arxiv.org/abs/2308.08469) | Arxiv 2023 | LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs | None |\n",
    "| 23-10-03 | [Time-LLM](https://arxiv.org/abs/2310.01728) | Arxiv 2023 | Time-LLM: Time Series Forecasting by Reprogramming Large Language Models | None |\n",
    "| 23-10-08 | [TEMPO](https://arxiv.org/abs/2310.04948) | Arxiv 2023 | TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting | None |\n",
    "| 23-10-12 | [Lag-Llama](https://arxiv.org/abs/2310.08278) | Arxiv 2023 | Lag-Llama: Towards Foundation Models for Time Series Forecasting | [Lag-Llama](https://github.com/kashif/pytorch-transformer-ts) |\n",
    "| 23-10-15 | [UniTime](https://arxiv.org/abs/2310.09751) | Arxiv 2023 | UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting | None |\n",
    "\n",
    "2. 复现两篇 time series forcasting 论文，使用计算机的 ETT 数据集，一篇是基于 LLM 的，一篇是基于卷积的 SOTA 方法，得到了和论文汇报差不多的指标结果。\n",
    "\n",
    "| Method | Conference | Paper Title |\n",
    "| ---- | ---- | ---- |\n",
    "| FPT | NIPS 2023 | [One Fits All:Power General Time Series Analysis by Pretrained LM](https://arxiv.org/abs/2302.11939) | \n",
    "| TimesNet | ICLR 2023 | [TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis](https://arxiv.org/abs/2210.02186) |\n",
    "\n",
    "### 【下周工作计划】\n",
    "1. 抽取手上 120w+ 条 Kepler 光变曲线的时间序列数据，清洗整理汇集，便于后续模型训练，避免超大规模的零散小文件加载拖慢训练时间。\n",
    "2. 调整 Kepler 曲线数据集适配计算机的时间序列预测任务，LLM 模型是否会取得不错的效果。如果可以，更大的 LLM 和自监督的方法是否可以取得更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023.10.23 - 2023.10.29\n",
    "\n",
    "### 【本周工作总结】\n",
    "1. 抽取手上 120w+ 条 Kepler 光变曲线的时间序列数据，清洗整理汇集。这样的数据量在整个计算机界都是稀缺的，先用一部分实验，未来考虑将整个kepler做成便于加载的数据集，测试多种方法，弄一个benchmarking。\n",
    "\n",
    "### 【下周工作计划】\n",
    "1. GPT(6)模型在 Kepler 数据集上测试，预训练和随机初始化对比效果，目的：LLM 冻结权重要比随机初始化表现好，这真的是因为模型里有知识吗，还是说只是现有数据集不够大，随机初始化的模型没有充分训练，所以表现不好。\n",
    "2. 调研 irregular time series analysis 的论文，这是之前漏掉的一个研究方向，比较小众，对于天文观测采样不均匀的特点来说，可能会有作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script processes astronomical light curve data stored in FITS files, \n",
    "    leveraging multiprocessing for enhanced performance.\n",
    "\n",
    "It normalizes the time and flux data and saves the processed data into CSV files.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(file_path, save_root):\n",
    "    \"\"\"\n",
    "    Process a single FITS file: read, normalize data, and save as CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fits.open(file_path) as f:\n",
    "            # Extracting essential information from the header\n",
    "            header = f[0].header\n",
    "            kepid = header.get(\"KEPLERID\")\n",
    "            quarter = header.get(\"QUARTER\")\n",
    "\n",
    "            # Time normalization\n",
    "            bkjd = f[1].data[\"TIME\"]\n",
    "            bkjd_nan = np.isnan(bkjd)\n",
    "            bkjd[bkjd_nan] = np.interp(np.flatnonzero(bkjd_nan), np.flatnonzero(~bkjd_nan), bkjd[~bkjd_nan])\n",
    "            jd = bkjd + 2454833.0  # Convert BKJD to JD\n",
    "            date = Time(jd, format=\"jd\").to_datetime()\n",
    "            date = pd.to_datetime(pd.Series(date)).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            # Flux normalization\n",
    "            flux = f[1].data[\"PDCSAP_FLUX\"]\n",
    "            median = np.nanmedian(flux)\n",
    "            q1 = np.nanpercentile(flux, 25)\n",
    "            q3 = np.nanpercentile(flux, 75)\n",
    "            iqr = q3 - q1\n",
    "            iqr = 1 if iqr == 0 else iqr  # Avoid division by zero\n",
    "            flux = (flux - median) / iqr  # Scaling\n",
    "\n",
    "            # Preparing DataFrame and saving as CSV\n",
    "            df = pd.DataFrame({\"date\": date, \"flux\": flux})\n",
    "            df.set_index(\"date\", inplace=True)\n",
    "            dir_structure = os.path.dirname(file_path).split(os.sep)[-2:]\n",
    "            save_dir = os.path.join(save_root, *dir_structure)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(save_dir, f\"{str(kepid).zfill(9)}-{str(quarter).zfill(2)}.csv\")\n",
    "            df.to_csv(save_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}. Error: {e}\")\n",
    "\n",
    "def main(root, save_root):\n",
    "    \"\"\"\n",
    "    Execute the processing of FITS files using multiprocessing.\n",
    "    \"\"\"\n",
    "    files_to_process = []\n",
    "\n",
    "    # Walk through the directory tree and collect all files to process\n",
    "    for dir, subdirs, files in os.walk(root):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(dir, file)\n",
    "            files_to_process.append((file_path, save_root))\n",
    "\n",
    "    # Multiprocessing execution setup\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # This part is adjusted to work with tqdm, ensuring that the progress bar is displayed correctly.\n",
    "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "        # Prepare the progress bar\n",
    "        progress_bar = tqdm(total=len(files_to_process), desc=\"Processing files\", unit=\"file\")\n",
    "\n",
    "        # We use a list to keep all futures, and we use it to know when a future is completed.\n",
    "        futures = [executor.submit(process_file, *task) for task in files_to_process]\n",
    "\n",
    "        for _ in as_completed(futures):  # as each future completes, it returns the result.\n",
    "            progress_bar.update(1)  # Update the progress bar by 1 for each completed task.\n",
    "\n",
    "        progress_bar.close()  # Don't forget to close the progress bar at the end.\n",
    "\n",
    "# Paths for the source and destination directories\n",
    "root = \"/media/jmh/T7/kepler_lc\"\n",
    "save_root = \"/home/jmh/Datasets/kepler_csv/\"\n",
    "\n",
    "# Running the main processing function\n",
    "main(root, save_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed Compressed /home/jmh/Datasets/kepler_csv/0060 into /home/jmh/Datasets/kepler_csv_compressed/0060.tar.gz: 100%|██████████| 119/119 [4:51:30<00:00, 146.98s/dir]  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script: Compress Subdirectories\n",
    "\n",
    "Description:\n",
    "    This script is used to compress the subdirectories located in a specified \"original\" root directory.\n",
    "    Each subdirectory corresponds to a 'sub_id' and is compressed into an individual .tar.gz file.\n",
    "    The resulting compressed files are saved in a \"new\" specified root directory.\n",
    "\n",
    "    The script assumes that directories under the original root are structured as:\n",
    "    original_root/sub_id\n",
    "\n",
    "    The script does not recursively compress subdirectories beyond the immediate children\n",
    "    of the original root directory. The script only targets directories, ignoring any\n",
    "    files present in the original root directory.\n",
    "\n",
    "Usage:\n",
    "    The script is executed in a Python environment and does not require any command-line arguments.\n",
    "    Users must modify the 'original_root_dir' and 'new_root_dir' variables to set the paths\n",
    "    for source and destination directories, respectively.\n",
    "\n",
    "    original_root_dir = \"/path/to/original/root\"  # path to the source directory\n",
    "    new_root_dir = \"/path/to/new/root\"  # path to the destination directory\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compress_directory(sub_id_path, new_root):\n",
    "    \"\"\"\n",
    "    Compress the contents of 'sub_id_path' and save it in 'new_root'.\n",
    "\n",
    "    :param sub_id_path: Full path to the subdirectory.\n",
    "    :param new_root: Directory where the .tar.gz files will be saved.\n",
    "    \"\"\"\n",
    "    sub_id = os.path.basename(sub_id_path)\n",
    "    tar_path = os.path.join(new_root, f\"{sub_id}.tar.gz\")\n",
    "\n",
    "    # Open tarball file for writing with gzip compression\n",
    "    with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "        # Walk through each file in the sub_id directory\n",
    "        for dirpath, dirnames, filenames in os.walk(sub_id_path):\n",
    "            for file in filenames:\n",
    "                # Construct the absolute path of the file\n",
    "                absolute_file_path = os.path.join(dirpath, file)\n",
    "\n",
    "                # Identify the relative path of the file, which should be the path within the tarball\n",
    "                # This step is crucial to avoid including unnecessary path information in the tarball\n",
    "                arcname = os.path.relpath(absolute_file_path, start=sub_id_path)\n",
    "\n",
    "                # Add the file with its relative path information to the tarball\n",
    "                tar.add(absolute_file_path, arcname=arcname)\n",
    "\n",
    "    return f\"Compressed {sub_id_path} into {tar_path}\"  # Return status\n",
    "\n",
    "\n",
    "def main(original_root, new_root):\n",
    "    # Create new root if it doesn't exist.\n",
    "    if not os.path.exists(new_root):\n",
    "        os.makedirs(new_root)\n",
    "\n",
    "    # Construct a list of directories to be compressed\n",
    "    dirs_to_compress = [os.path.join(original_root, sub_id) for sub_id in os.listdir(original_root) \\\n",
    "                         if os.path.isdir(os.path.join(original_root, sub_id))]\n",
    "    \n",
    "    # Number of directories to compress, for the progress bar\n",
    "    num_dirs = len(dirs_to_compress)\n",
    "\n",
    "    # We use a ThreadPoolExecutor to compress directories in separate threads.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        # Setup tqdm to track the task's progress\n",
    "        progress = tqdm(\n",
    "            concurrent.futures.as_completed([\n",
    "                executor.submit(compress_directory, dir_path, new_root) for dir_path in dirs_to_compress\n",
    "            ]), \n",
    "            total=num_dirs, \n",
    "            unit=\"dir\", \n",
    "            desc=\"Compressing\", \n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        for future in progress:\n",
    "            try:\n",
    "                # Fetch the result and update the progress bar description\n",
    "                result = future.result()\n",
    "                progress.set_description(f\"Processed {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred: {e}\")\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress.close()\n",
    "\n",
    "# Set your original and new root directories.\n",
    "original_root_dir = \"/home/jmh/Datasets/kepler_csv/\"\n",
    "new_root_dir = \"/home/jmh/Datasets/kepler_csv_compressed/\"\n",
    "\n",
    "main(original_root_dir, new_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/1282116 [00:00<?, ?file/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-Process Time Series Data Cleaning Script\n",
    "\n",
    "This script is designed to process large sets of time series data stored in CSV files.\n",
    "It performs the following operations:\n",
    "1. Removes any rows with NaNs at the beginning and end of the time series data.\n",
    "2. Applies third-order spline interpolation for missing values within the series.\n",
    "3. Saves the cleaned data back to CSV, replacing the original files.\n",
    "\n",
    "The script leverages multiprocessing for enhanced performance, particularly when dealing\n",
    "with a large number of files. It also implements progress tracking, giving real-time feedback\n",
    "on the number of files processed.\n",
    "\n",
    "Usage:\n",
    "    Define the root directory containing the CSV files and the save directory for processed files.\n",
    "    Run the script, and it will process files located in 'root/sub_id/id/*.csv'.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import splrep, splev\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_time_series(file_path, save_root):\n",
    "    \"\"\"\n",
    "    Processes a single time series CSV file, handling missing data and saving the modified file.\n",
    "    \n",
    "    :param file_path: Path to the original CSV file.\n",
    "    :param save_root: Root directory where the processed file will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and prepare the data\n",
    "        data = pd.read_csv(file_path)\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data.set_index('date', inplace=True)\n",
    "        data.sort_index(inplace=True)\n",
    "        \n",
    "        # 1. Remove NaNs at the beginning and the end\n",
    "        # Identify valid data points (non-NaN) from start and end\n",
    "        valid_start = data['flux'].first_valid_index()\n",
    "        valid_end = data['flux'].last_valid_index()\n",
    "\n",
    "        # Drop the rows with NaNs at the beginning and end of the dataframe\n",
    "        if valid_start is not None and valid_end is not None:\n",
    "            data = data.loc[valid_start:valid_end]\n",
    "        else:\n",
    "            # If the entire series is NaN, we skip the file\n",
    "            print(f\"File {file_path} was skipped as it contains all NaNs.\")\n",
    "            return\n",
    "        \n",
    "        # 2. Spline interpolation for NaNs in the middle of the series\n",
    "        # Identify where the NaNs are in the series\n",
    "        nan_positions = data['flux'].isna()\n",
    "\n",
    "        # If there are NaNs to interpolate\n",
    "        if nan_positions.any():\n",
    "            # Extract the positions (indices) and values for non-NaN elements\n",
    "            x = np.where(~nan_positions)[0]\n",
    "            y = data.loc[~nan_positions, 'flux']\n",
    "\n",
    "            # Create a spline of degree 3 (cubic spline)\n",
    "            spline = splrep(x, y, k=3)\n",
    "\n",
    "            # Identify the positions where we have NaNs\n",
    "            x_interp_positions = np.where(nan_positions)[0]\n",
    "\n",
    "            # Calculate the interpolated values at these positions\n",
    "            interpolated_values = splev(x_interp_positions, spline)\n",
    "\n",
    "            # Replace NaNs with interpolated values in the dataframe\n",
    "            data.loc[nan_positions, 'flux'] = interpolated_values\n",
    "        \n",
    "        # Construct the save path from the original file path structure\n",
    "        dir_structure = os.path.dirname(file_path).split(os.sep)[-2:]\n",
    "        save_dir = os.path.join(save_root, *dir_structure)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        save_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "        # Save the cleaned data\n",
    "        data.to_csv(save_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "def main(root, save_root):\n",
    "    \"\"\"\n",
    "    Orchestrates the multiprocessing execution for cleaning time series data.\n",
    "    \n",
    "    :param root: Root directory containing the unprocessed CSV files.\n",
    "    :param save_root: Root directory where processed files will be stored.\n",
    "    \"\"\"\n",
    "    # Retrieve file paths\n",
    "    files_to_process = [os.path.join(dir, file) \n",
    "                        for dir, _, files in os.walk(root) for file in files if file.endswith(\".csv\")]\n",
    "\n",
    "    # Setup for multiprocessing\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tqdm(total=len(files_to_process), desc=\"Processing files\", unit=\"file\")\n",
    "\n",
    "        # Submit tasks to the process pool\n",
    "        futures = [executor.submit(process_time_series, file_path, save_root) for file_path in files_to_process]\n",
    "\n",
    "        # Update progress for each completed task\n",
    "        for _ in as_completed(futures):\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.refresh()\n",
    "\n",
    "        # Cleanup\n",
    "        progress_bar.close() \n",
    "\n",
    "# Specify source and destination directories\n",
    "root = \"/home/jmh/Datasets/kepler/\"  # Root directory with original CSV files\n",
    "save_root = \"/home/jmh/Datasets/kepler_filled/\"  # Destination for processed data\n",
    "\n",
    "# Start the processing\n",
    "main(root, save_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
