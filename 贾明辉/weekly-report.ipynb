{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å·¥ä½œå‘¨æŠ¥\n",
    "\n",
    "## 2023.10.22\n",
    "\n",
    "### ã€æœ¬å‘¨å·¥ä½œæ€»ç»“ã€‘\n",
    "1. è°ƒç ”æ—¶é—´åºåˆ— tokenizer æ–¹å¼é€‚é…å¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚\n",
    "\n",
    "Date|Method|Conference| Paper Title and Paper Interpretation (In Chinese) |Code\n",
    "-----|----|-----|-----|-----\n",
    "| 23-02-23 | [FPT](https://arxiv.org/abs/2302.11939) ğŸŒŸ | NIPS 2023 | One Fits All:Power General Time Series Analysis by Pretrained LM | [One-Fits-All](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All)   |\n",
    "| 23-05-17 | [LLMTime](https://arxiv.org/abs/2310.07820) | NIPS 2023 | Large Language Models Are Zero-Shot Time Series Forecasters | [LLMTime](https://github.com/ngruver/llmtime) |\n",
    "| 23-08-16 | [TEST](https://arxiv.org/abs/2308.08241) | Arxiv 2023 | TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series | None |\n",
    "| 23-08-16 | [LLM4TS](https://arxiv.org/abs/2308.08469) | Arxiv 2023 | LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs | None |\n",
    "| 23-10-03 | [Time-LLM](https://arxiv.org/abs/2310.01728) | Arxiv 2023 | Time-LLM: Time Series Forecasting by Reprogramming Large Language Models | None |\n",
    "| 23-10-08 | [TEMPO](https://arxiv.org/abs/2310.04948) | Arxiv 2023 | TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting | None |\n",
    "| 23-10-12 | [Lag-Llama](https://arxiv.org/abs/2310.08278) | Arxiv 2023 | Lag-Llama: Towards Foundation Models for Time Series Forecasting | [Lag-Llama](https://github.com/kashif/pytorch-transformer-ts) |\n",
    "| 23-10-15 | [UniTime](https://arxiv.org/abs/2310.09751) | Arxiv 2023 | UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting | None |\n",
    "\n",
    "2. å¤ç°ä¸¤ç¯‡ time series forcasting è®ºæ–‡ï¼Œä½¿ç”¨è®¡ç®—æœºçš„ ETT æ•°æ®é›†ï¼Œä¸€ç¯‡æ˜¯åŸºäº LLM çš„ï¼Œä¸€ç¯‡æ˜¯åŸºäºå·ç§¯çš„ SOTA æ–¹æ³•ï¼Œå¾—åˆ°äº†å’Œè®ºæ–‡æ±‡æŠ¥å·®ä¸å¤šçš„æŒ‡æ ‡ç»“æœã€‚\n",
    "\n",
    "| Method | Conference | Paper Title |\n",
    "| ---- | ---- | ---- |\n",
    "| FPT | NIPS 2023 | [One Fits All:Power General Time Series Analysis by Pretrained LM](https://arxiv.org/abs/2302.11939) | \n",
    "| TimesNet | ICLR 2023 | [TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis](https://arxiv.org/abs/2210.02186) |\n",
    "\n",
    "### ã€ä¸‹å‘¨å·¥ä½œè®¡åˆ’ã€‘\n",
    "1. æŠ½å–æ‰‹ä¸Š 120w+ æ¡ Kepler å…‰å˜æ›²çº¿çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œæ¸…æ´—æ•´ç†æ±‡é›†ï¼Œä¾¿äºåç»­æ¨¡å‹è®­ç»ƒï¼Œé¿å…è¶…å¤§è§„æ¨¡çš„é›¶æ•£å°æ–‡ä»¶åŠ è½½æ‹–æ…¢è®­ç»ƒæ—¶é—´ã€‚\n",
    "2. è°ƒæ•´ Kepler æ›²çº¿æ•°æ®é›†é€‚é…è®¡ç®—æœºçš„æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ï¼ŒLLM æ¨¡å‹æ˜¯å¦ä¼šå–å¾—ä¸é”™çš„æ•ˆæœã€‚å¦‚æœå¯ä»¥ï¼Œæ›´å¤§çš„ LLM å’Œè‡ªç›‘ç£çš„æ–¹æ³•æ˜¯å¦å¯ä»¥å–å¾—æ›´å¥½çš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023.10.23 - 2023.10.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script processes astronomical light curve data stored in FITS files, \n",
    "    leveraging multiprocessing for enhanced performance.\n",
    "\n",
    "It normalizes the time and flux data and saves the processed data into CSV files.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(file_path, save_root):\n",
    "    \"\"\"\n",
    "    Process a single FITS file: read, normalize data, and save as CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fits.open(file_path) as f:\n",
    "            # Extracting essential information from the header\n",
    "            header = f[0].header\n",
    "            kepid = header.get(\"KEPLERID\")\n",
    "            quarter = header.get(\"QUARTER\")\n",
    "\n",
    "            # Time normalization\n",
    "            bkjd = f[1].data[\"TIME\"]\n",
    "            bkjd_nan = np.isnan(bkjd)\n",
    "            bkjd[bkjd_nan] = np.interp(np.flatnonzero(bkjd_nan), np.flatnonzero(~bkjd_nan), bkjd[~bkjd_nan])\n",
    "            jd = bkjd + 2454833.0  # Convert BKJD to JD\n",
    "            date = Time(jd, format=\"jd\").to_datetime()\n",
    "            date = pd.to_datetime(pd.Series(date)).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            # Flux normalization\n",
    "            flux = f[1].data[\"PDCSAP_FLUX\"]\n",
    "            median = np.nanmedian(flux)\n",
    "            q1 = np.nanpercentile(flux, 25)\n",
    "            q3 = np.nanpercentile(flux, 75)\n",
    "            iqr = q3 - q1\n",
    "            iqr = 1 if iqr == 0 else iqr  # Avoid division by zero\n",
    "            flux = (flux - median) / iqr  # Scaling\n",
    "\n",
    "            # Preparing DataFrame and saving as CSV\n",
    "            df = pd.DataFrame({\"date\": date, \"flux\": flux})\n",
    "            df.set_index(\"date\", inplace=True)\n",
    "            dir_structure = os.path.dirname(file_path).split(os.sep)[-2:]\n",
    "            save_dir = os.path.join(save_root, *dir_structure)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(save_dir, f\"{str(kepid).zfill(9)}-{str(quarter).zfill(2)}.csv\")\n",
    "            df.to_csv(save_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}. Error: {e}\")\n",
    "\n",
    "def main(root, save_root):\n",
    "    \"\"\"\n",
    "    Execute the processing of FITS files using multiprocessing.\n",
    "    \"\"\"\n",
    "    files_to_process = []\n",
    "\n",
    "    # Walk through the directory tree and collect all files to process\n",
    "    for dir, subdirs, files in os.walk(root):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(dir, file)\n",
    "            files_to_process.append((file_path, save_root))\n",
    "\n",
    "    # Multiprocessing execution setup\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # This part is adjusted to work with tqdm, ensuring that the progress bar is displayed correctly.\n",
    "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "        # Prepare the progress bar\n",
    "        progress_bar = tqdm(total=len(files_to_process), desc=\"Processing files\", unit=\"file\")\n",
    "\n",
    "        # We use a list to keep all futures, and we use it to know when a future is completed.\n",
    "        futures = [executor.submit(process_file, *task) for task in files_to_process]\n",
    "\n",
    "        for _ in as_completed(futures):  # as each future completes, it returns the result.\n",
    "            progress_bar.update(1)  # Update the progress bar by 1 for each completed task.\n",
    "\n",
    "        progress_bar.close()  # Don't forget to close the progress bar at the end.\n",
    "\n",
    "# Paths for the source and destination directories\n",
    "root = \"/media/jmh/T7/kepler_lc\"\n",
    "save_root = \"/home/jmh/Datasets/kepler_csv/\"\n",
    "\n",
    "# Running the main processing function\n",
    "main(root, save_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed Compressed /home/jmh/Datasets/kepler_csv/0060 into /home/jmh/Datasets/kepler_csv_compressed/0060.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [4:51:30<00:00, 146.98s/dir]  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script: Compress Subdirectories\n",
    "\n",
    "Description:\n",
    "    This script is used to compress the subdirectories located in a specified \"original\" root directory.\n",
    "    Each subdirectory corresponds to a 'sub_id' and is compressed into an individual .tar.gz file.\n",
    "    The resulting compressed files are saved in a \"new\" specified root directory.\n",
    "\n",
    "    The script assumes that directories under the original root are structured as:\n",
    "    original_root/sub_id\n",
    "\n",
    "    The script does not recursively compress subdirectories beyond the immediate children\n",
    "    of the original root directory. The script only targets directories, ignoring any\n",
    "    files present in the original root directory.\n",
    "\n",
    "Usage:\n",
    "    The script is executed in a Python environment and does not require any command-line arguments.\n",
    "    Users must modify the 'original_root_dir' and 'new_root_dir' variables to set the paths\n",
    "    for source and destination directories, respectively.\n",
    "\n",
    "    original_root_dir = \"/path/to/original/root\"  # path to the source directory\n",
    "    new_root_dir = \"/path/to/new/root\"  # path to the destination directory\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compress_directory(sub_id_path, new_root):\n",
    "    \"\"\"\n",
    "    Compress the contents of 'sub_id_path' and save it in 'new_root'.\n",
    "\n",
    "    :param sub_id_path: Full path to the subdirectory.\n",
    "    :param new_root: Directory where the .tar.gz files will be saved.\n",
    "    \"\"\"\n",
    "    sub_id = os.path.basename(sub_id_path)\n",
    "    tar_path = os.path.join(new_root, f\"{sub_id}.tar.gz\")\n",
    "\n",
    "    # Open tarball file for writing with gzip compression\n",
    "    with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "        # Walk through each file in the sub_id directory\n",
    "        for dirpath, dirnames, filenames in os.walk(sub_id_path):\n",
    "            for file in filenames:\n",
    "                # Construct the absolute path of the file\n",
    "                absolute_file_path = os.path.join(dirpath, file)\n",
    "\n",
    "                # Identify the relative path of the file, which should be the path within the tarball\n",
    "                # This step is crucial to avoid including unnecessary path information in the tarball\n",
    "                arcname = os.path.relpath(absolute_file_path, start=sub_id_path)\n",
    "\n",
    "                # Add the file with its relative path information to the tarball\n",
    "                tar.add(absolute_file_path, arcname=arcname)\n",
    "\n",
    "    return f\"Compressed {sub_id_path} into {tar_path}\"  # Return status\n",
    "\n",
    "\n",
    "def main(original_root, new_root):\n",
    "    # Create new root if it doesn't exist.\n",
    "    if not os.path.exists(new_root):\n",
    "        os.makedirs(new_root)\n",
    "\n",
    "    # Construct a list of directories to be compressed\n",
    "    dirs_to_compress = [os.path.join(original_root, sub_id) for sub_id in os.listdir(original_root) \\\n",
    "                         if os.path.isdir(os.path.join(original_root, sub_id))]\n",
    "    \n",
    "    # Number of directories to compress, for the progress bar\n",
    "    num_dirs = len(dirs_to_compress)\n",
    "\n",
    "    # We use a ThreadPoolExecutor to compress directories in separate threads.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        # Setup tqdm to track the task's progress\n",
    "        progress = tqdm(\n",
    "            concurrent.futures.as_completed([\n",
    "                executor.submit(compress_directory, dir_path, new_root) for dir_path in dirs_to_compress\n",
    "            ]), \n",
    "            total=num_dirs, \n",
    "            unit=\"dir\", \n",
    "            desc=\"Compressing\", \n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        for future in progress:\n",
    "            try:\n",
    "                # Fetch the result and update the progress bar description\n",
    "                result = future.result()\n",
    "                progress.set_description(f\"Processed {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred: {e}\")\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress.close()\n",
    "\n",
    "# Set your original and new root directories.\n",
    "original_root_dir = \"/home/jmh/Datasets/kepler_csv/\"\n",
    "new_root_dir = \"/home/jmh/Datasets/kepler_csv_compressed/\"\n",
    "\n",
    "main(original_root_dir, new_root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
