{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å·¥ä½œå‘¨æŠ¥\n",
    "\n",
    "## 2023.10.22\n",
    "\n",
    "### ã€æœ¬å‘¨å·¥ä½œæ€»ç»“ã€‘\n",
    "1. è°ƒç ”æ—¶é—´åºåˆ— tokenizer æ–¹å¼é€‚é…å¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚\n",
    "\n",
    "Date|Method|Conference| Paper Title and Paper Interpretation (In Chinese) |Code\n",
    "-----|----|-----|-----|-----\n",
    "| 23-02-23 | [FPT](https://arxiv.org/abs/2302.11939) ğŸŒŸ | NIPS 2023 | [One Fits All:Power General Time Series Analysis by Pretrained LM](https://zhuanlan.zhihu.com/p/661884836) | [One-Fits-All](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All)   |\n",
    "| 23-05-17 | [LLMTime](https://arxiv.org/abs/2310.07820) | NIPS 2023 | [Large Language Models Are Zero-Shot Time Series Forecasters](https://zhuanlan.zhihu.com/p/661526823) | [LLMTime](https://github.com/ngruver/llmtime) |\n",
    "| 23-08-16 | [TEST](https://arxiv.org/abs/2308.08241) | Arxiv 2023 | TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series | None |\n",
    "| 23-08-16 | [LLM4TS](https://arxiv.org/abs/2308.08469) | Arxiv 2023 | LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs | None |\n",
    "| 23-10-03 | [Time-LLM](https://arxiv.org/abs/2310.01728) | Arxiv 2023 | Time-LLM: Time Series Forecasting by Reprogramming Large Language Models | None |\n",
    "| 23-10-08 | [TEMPO](https://arxiv.org/abs/2310.04948) | Arxiv 2023 | TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting | None |\n",
    "| 23-10-12 | [Lag-Llama](https://arxiv.org/abs/2310.08278) | Arxiv 2023 | Lag-Llama: Towards Foundation Models for Time Series Forecasting | [Lag-Llama](https://github.com/kashif/pytorch-transformer-ts) |\n",
    "| 23-10-15 | [UniTime](https://arxiv.org/abs/2310.09751) | Arxiv 2023 | UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting | None |\n",
    "\n",
    "2. å¤ç°ä¸¤ç¯‡ time series forcasting è®ºæ–‡ï¼Œä½¿ç”¨è®¡ç®—æœºçš„ ETT æ•°æ®é›†ï¼Œä¸€ç¯‡æ˜¯åŸºäº LLM çš„ï¼Œä¸€ç¯‡æ˜¯åŸºäºå·ç§¯çš„ SOTA æ–¹æ³•ï¼Œå¾—åˆ°äº†å’Œè®ºæ–‡æ±‡æŠ¥å·®ä¸å¤šçš„æŒ‡æ ‡ç»“æœã€‚\n",
    "\n",
    "| Method | Conference | Paper Title |\n",
    "| ---- | ---- | ---- |\n",
    "| FPT | NIPS 2023 | [One Fits All:Power General Time Series Analysis by Pretrained LM](https://arxiv.org/abs/2302.11939) | \n",
    "| TimesNet | ICLR 2023 | [TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis](https://arxiv.org/abs/2210.02186) |\n",
    "\n",
    "### ã€ä¸‹å‘¨å·¥ä½œè®¡åˆ’ã€‘\n",
    "1. æŠ½å–æ‰‹ä¸Š 120w+ æ¡ Kepler å…‰å˜æ›²çº¿çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œæ¸…æ´—æ•´ç†æ±‡é›†ï¼Œä¾¿äºåç»­æ¨¡å‹è®­ç»ƒï¼Œé¿å…è¶…å¤§è§„æ¨¡çš„é›¶æ•£å°æ–‡ä»¶åŠ è½½æ‹–æ…¢è®­ç»ƒæ—¶é—´ã€‚\n",
    "2. è°ƒæ•´ Kepler æ›²çº¿æ•°æ®é›†é€‚é…è®¡ç®—æœºçš„æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ï¼ŒLLM æ¨¡å‹æ˜¯å¦ä¼šå–å¾—ä¸é”™çš„æ•ˆæœã€‚å¦‚æœå¯ä»¥ï¼Œæ›´å¤§çš„ LLM å’Œè‡ªç›‘ç£çš„æ–¹æ³•æ˜¯å¦å¯ä»¥å–å¾—æ›´å¥½çš„æ•ˆæœã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
