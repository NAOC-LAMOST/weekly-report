{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工作周报\n",
    "\n",
    "## 2023.10.22\n",
    "\n",
    "### 【本周工作总结】\n",
    "1. 调研时间序列 tokenizer 方式适配大语言模型的方法。\n",
    "\n",
    "Date|Method|Conference| Paper Title and Paper Interpretation (In Chinese) |Code\n",
    "-----|----|-----|-----|-----\n",
    "| 23-02-23 | [FPT](https://arxiv.org/abs/2302.11939) 🌟 | NIPS 2023 | [One Fits All:Power General Time Series Analysis by Pretrained LM](https://zhuanlan.zhihu.com/p/661884836) | [One-Fits-All](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All)   |\n",
    "| 23-05-17 | [LLMTime](https://arxiv.org/abs/2310.07820) | NIPS 2023 | [Large Language Models Are Zero-Shot Time Series Forecasters](https://zhuanlan.zhihu.com/p/661526823) | [LLMTime](https://github.com/ngruver/llmtime) |\n",
    "| 23-08-16 | [TEST](https://arxiv.org/abs/2308.08241) | Arxiv 2023 | TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series | None |\n",
    "| 23-08-16 | [LLM4TS](https://arxiv.org/abs/2308.08469) | Arxiv 2023 | LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs | None |\n",
    "| 23-10-03 | [Time-LLM](https://arxiv.org/abs/2310.01728) | Arxiv 2023 | Time-LLM: Time Series Forecasting by Reprogramming Large Language Models | None |\n",
    "| 23-10-08 | [TEMPO](https://arxiv.org/abs/2310.04948) | Arxiv 2023 | TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting | None |\n",
    "| 23-10-12 | [Lag-Llama](https://arxiv.org/abs/2310.08278) | Arxiv 2023 | Lag-Llama: Towards Foundation Models for Time Series Forecasting | [Lag-Llama](https://github.com/kashif/pytorch-transformer-ts) |\n",
    "| 23-10-15 | [UniTime](https://arxiv.org/abs/2310.09751) | Arxiv 2023 | UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting | None |\n",
    "\n",
    "2. 复现两篇 time series forcasting 论文，使用计算机的 ETT 数据集，一篇是基于 LLM 的，一篇是基于卷积的 SOTA 方法，得到了和论文汇报差不多的指标结果。\n",
    "\n",
    "| Method | Conference | Paper Title |\n",
    "| ---- | ---- | ---- |\n",
    "| FPT | NIPS 2023 | [One Fits All:Power General Time Series Analysis by Pretrained LM](https://arxiv.org/abs/2302.11939) | \n",
    "| TimesNet | ICLR 2023 | [TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis](https://arxiv.org/abs/2210.02186) |\n",
    "\n",
    "### 【下周工作计划】\n",
    "1. 抽取手上 120w+ 条 Kepler 光变曲线的时间序列数据，清洗整理汇集，便于后续模型训练，避免超大规模的零散小文件加载拖慢训练时间。\n",
    "2. 调整 Kepler 曲线数据集适配计算机的时间序列预测任务，LLM 模型是否会取得不错的效果。如果可以，更大的 LLM 和自监督的方法是否可以取得更好的效果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
